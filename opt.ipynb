{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 4057,
     "status": "ok",
     "timestamp": 1652029919193,
     "user": {
      "displayName": "김주엽",
      "userId": "06751949334461059175"
     },
     "user_tz": -540
    },
    "id": "KuEQxQC0gs5d"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import prod\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, concatenate, Dropout, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "NMS_THRESHOLD=0.3\n",
    "MIN_CONFIDENCE=0.2\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0\"  # Set the GPU 2 to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pedestrian_detection(image, model, layer_name, personidz=0):\n",
    "\t(H, W) = image.shape[:2]\n",
    "\tresults = []\n",
    "\n",
    "\n",
    "\tblob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416),\n",
    "\t\tswapRB=True, crop=False)\n",
    "\tmodel.setInput(blob)\n",
    "\tlayerOutputs = model.forward(layer_name)\n",
    "\n",
    "\tboxes = []\n",
    "\tcentroids = []\n",
    "\tconfidences = []\n",
    "\n",
    "\tfor output in layerOutputs:\n",
    "\t\tfor detection in output:\n",
    "\n",
    "\t\t\tscores = detection[5:]\n",
    "\t\t\tclassID = np.argmax(scores)\n",
    "\t\t\tconfidence = scores[classID]\n",
    "\n",
    "\t\t\tif classID == personidz and confidence > MIN_CONFIDENCE:\n",
    "\n",
    "\t\t\t\tbox = detection[0:4] * np.array([W, H, W, H])\n",
    "\t\t\t\t(centerX, centerY, width, height) = box.astype(\"int\")\n",
    "\n",
    "\t\t\t\tx = int(centerX - (width / 2))\n",
    "\t\t\t\ty = int(centerY - (height / 2))\n",
    "\n",
    "\t\t\t\tboxes.append([x, y, int(width), int(height)])\n",
    "\t\t\t\tcentroids.append((centerX, centerY))\n",
    "\t\t\t\tconfidences.append(float(confidence))\n",
    "\t# apply non-maxima suppression to suppress weak, overlapping\n",
    "\t# bounding boxes\n",
    "\tidzs = cv2.dnn.NMSBoxes(boxes, confidences, MIN_CONFIDENCE, NMS_THRESHOLD)\n",
    "\t# ensure at least one detection exists\n",
    "\tif len(idzs) > 0:\n",
    "\t\t# loop over the indexes we are keeping\n",
    "\t\tfor i in idzs.flatten():\n",
    "\t\t\t# extract the bounding box coordinates\n",
    "\t\t\t(x, y) = (boxes[i][0], boxes[i][1])\n",
    "\t\t\t(w, h) = (boxes[i][2], boxes[i][3])\n",
    "\t\t\t# update our results list to consist of the person\n",
    "\t\t\t# prediction probability, bounding box coordinates,\n",
    "\t\t\t# and the centroid\n",
    "\t\t\tres = (confidences[i], (x, y, x + w, y + h), centroids[i])\n",
    "\t\t\tresults.append(res)\n",
    "\t# return the list of results\n",
    "\treturn results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1652029919563,
     "user": {
      "displayName": "김주엽",
      "userId": "06751949334461059175"
     },
     "user_tz": -540
    },
    "id": "uhxxTg-pgyLx"
   },
   "outputs": [],
   "source": [
    "#location = new_datasets/train/1\n",
    "#file_name = features_train_fight\n",
    "def preprocess(location, save_path, file_name):\n",
    "    \n",
    "    print(IMG_SIZE)\n",
    "    base = tf.keras.applications.MobileNetV3Small(input_shape=(IMG_SIZE, IMG_SIZE, 3), weights='imagenet', include_top=False)\n",
    "    features = np.empty((0, 28224))\n",
    "    labels = []\n",
    "    base_model = tf.keras.Sequential([\n",
    "      base,\n",
    "      Flatten()\n",
    "    ])\n",
    "    base_model.summary()\n",
    "    base_model.trainable = False\n",
    "    index = 0\n",
    "    label = location[-1]\n",
    "    labelsPath = \"coco.names\"\n",
    "    LABELS = open(labelsPath).read().strip().split(\"\\n\")\n",
    "\n",
    "    weights_path = \"yolov4-tiny.weights\"\n",
    "    config_path = \"yolov4-tiny.cfg\"\n",
    "\n",
    "    model = cv2.dnn.readNetFromDarknet(config_path, weights_path)\n",
    "    layer_name = model.getLayerNames()\n",
    "    layer_name = [layer_name[i - 1] for i in model.getUnconnectedOutLayers()]\n",
    "    writer = None\n",
    "    \n",
    "    \n",
    "    \n",
    "    for lb, i in enumerate(os.listdir(location)): #location = new_datasets/train/1\n",
    "        filedir = os.path.join(location, i)\n",
    "        for j in os.listdir(filedir):\n",
    "            split_video = os.path.join(filedir, j)\n",
    "            frames = np.array(list(map(lambda x: os.path.join(split_video, x), os.listdir(split_video))))\n",
    "            frames = sorted(frames, key=lambda x: int(x.split(\"/\")[-1].strip(\".jpg\")))\n",
    "            if len(frames) > 3000:\n",
    "                print(\"frames over : \", len(frames))\n",
    "                n = len(frames) - 3000\n",
    "                n = n//2\n",
    "                frames = frames[n:-n]\n",
    "                \n",
    "#             cutline = len(frames) % LSTM_INTERVAL\n",
    "#             cutline -= 1\n",
    "#             if cutline:\n",
    "#                 frames = frames[:-cutline]\n",
    "            \n",
    "#             if os.path.isfile(save_path+'/'+file_name+'_'+str(index)+'.npy'):\n",
    "#                 index+=len(frames)// (LSTM_INTERVAL+1)\n",
    "#                 continue\n",
    "\n",
    "            labels.append([label]*(len(frames)))\n",
    "\n",
    "            frames = np.array(list(map(lambda x:  cv2.resize(cv2.cvtColor(cv2.imread(x), cv2.COLOR_BGR2RGB),(IMG_SIZE, IMG_SIZE)), frames)))\n",
    "            #             frames = np.array(list(map(lambda x: cv2.resize(x, (IMG_SIZE, IMG_SIZE)), frames)))\n",
    "    #             if len(frames) > 5000:\n",
    "    #                 n = len(frames) - 5000\n",
    "    #                 n = n//2\n",
    "    #                 frames = frames[:]\n",
    "            \n",
    "\n",
    "            if len(frames) < (LSTM_INTERVAL + 1):\n",
    "                continue\n",
    "            \n",
    "            mask_list = []\n",
    "            for frame in frames:\n",
    "                results = pedestrian_detection(frame, model, layer_name,personidz=LABELS.index(\"person\"))\n",
    "                mask = np.zeros(frame.shape[:2], np.uint8)\n",
    "                for res in results:\n",
    "                    polygon = np.array([[res[1][0]-10,res[1][1]-10],[res[1][2]+10,res[1][1]-10],[res[1][2]+10,res[1][3]+10],[res[1][0]-10,res[1][3]+10]])\n",
    "                    cv2.fillPoly(mask,[polygon],1)\n",
    "#                 plt.figure(figsize=(20,20))\n",
    "#                 plt.imshow(frame)\n",
    "#                 plt.show()\n",
    "                mask_list.append(mask.astype(bool))\n",
    "#                frame = frame * mask[:, :, np.newaxis]\n",
    "#                 temp_frames.append(frame)\n",
    "                \n",
    "\n",
    "            temp_frames = []\n",
    "                                                           \n",
    "#             print(\"len frames: \", len(frames))\n",
    "            for k in range(1, len(frames)):\n",
    "                if os.path.isfile(save_path+'/'+file_name+'_'+str(index)+'.npy'):\n",
    "                    index+=1\n",
    "                    continue\n",
    "                hsv = np.zeros_like(frames[k])\n",
    "                hsv[...,1] = 255\n",
    "                dst1 = frames[k-1]\n",
    "                dst2 = frames[k]                         \n",
    "                img_1 = cv2.cvtColor(dst1,cv2.COLOR_BGR2GRAY)\n",
    "                img_2 = cv2.cvtColor(dst2,cv2.COLOR_BGR2GRAY)\n",
    "                flow = cv2.calcOpticalFlowFarneback(img_1, img_2, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "                mag, ang = cv2.cartToPolar(flow[...,0], flow[...,1])\n",
    "                hsv[...,0] = ang*180/np.pi/2\n",
    "                hsv[...,2] = cv2.normalize(mag,None,0,255,cv2.NORM_MINMAX)\n",
    "                rgb = cv2.cvtColor(hsv,cv2.COLOR_HSV2BGR)\n",
    "#                 plt.figure(figsize=(20,20))\n",
    "#                 plt.imshow(rgb)\n",
    "#                 plt.show()\n",
    "                rgb = rgb * (mask_list[k-1][:, :, np.newaxis] + mask_list[k][:, :, np.newaxis])\n",
    "#                 plt.figure(figsize=(20,20))\n",
    "#                 plt.imshow(rgb)\n",
    "#                 plt.show()\n",
    "                temp_frames.append(rgb)\n",
    "            frames = np.array(temp_frames)\n",
    "#             print(\"len frames 2: \", len(frames))\n",
    "            feature_list = base_model.predict(frames)\n",
    "            for ind in range(0, len(feature_list), LSTM_INTERVAL):\n",
    "                np.save(save_path+'/'+file_name+'_'+str(index)+'.npy',feature_list[ind:ind+LSTM_INTERVAL])\n",
    "\n",
    "\n",
    "                index+=1\n",
    "        print(\"doing\", index)\n",
    "#         if index > 6000:\n",
    "#             break\n",
    "    print(\"done\")\n",
    "            #temp = np.concatenate([temp, feature_list])\n",
    "#             if temp.shape[0] > 5000:\n",
    "#                 features = np.concatenate([features, temp])\n",
    "#                 temp = np.empty((0, 28224))\n",
    "             #   print(features.shape)\n",
    "    #if temp.shape[0] >0:\n",
    "    #    features = np.concatenate([features, temp])\n",
    "    #print(features.shape)\n",
    "#     labels = sum(labels, [])\n",
    "#     return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_INTERVAL = 30\n",
    "IMG_SIZE = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " MobilenetV3small (Functiona  (None, 7, 7, 576)        939120    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 28224)             0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 939,120\n",
      "Trainable params: 927,008\n",
      "Non-trainable params: 12,112\n",
      "_________________________________________________________________\n",
      "frames over :  5157\n",
      "frames over :  3408\n",
      "frames over :  3825\n",
      "frames over :  3709\n",
      "frames over :  4129\n",
      "frames over :  3883\n",
      "frames over :  5187\n",
      "frames over :  4838\n",
      "frames over :  3249\n",
      "frames over :  3166\n",
      "frames over :  4349\n",
      "frames over :  3005\n",
      "frames over :  3904\n",
      "frames over :  4306\n",
      "frames over :  4004\n",
      "frames over :  3411\n",
      "frames over :  4255\n",
      "doing 74\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "location = \"원본/dataset/train/0\"\n",
    "save_path = \"fix_train_data/0/\"\n",
    "file_name = 'features_train_fight2'\n",
    "preprocess(location, save_path, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " MobilenetV3small (Functiona  (None, 7, 7, 576)        939120    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 28224)             0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 939,120\n",
      "Trainable params: 927,008\n",
      "Non-trainable params: 12,112\n",
      "_________________________________________________________________\n",
      "frames over :  8926\n",
      "frames over :  4162\n",
      "frames over :  9163\n",
      "frames over :  9036\n",
      "frames over :  8623\n",
      "frames over :  9060\n",
      "frames over :  8444\n",
      "frames over :  9157\n",
      "frames over :  7081\n",
      "frames over :  5934\n",
      "frames over :  7563\n",
      "frames over :  5391\n",
      "frames over :  4310\n",
      "frames over :  5588\n",
      "frames over :  9239\n",
      "frames over :  8069\n",
      "frames over :  7311\n",
      "frames over :  8548\n",
      "frames over :  8903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.5) /io/opencv/modules/imgproc/src/resize.cpp:4052: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1773252/459636486.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"fix_train_data/2/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'features_train_nomal2'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_1773252/3759238314.py\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(location, save_path, file_name)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0;31m#             frames = np.array(list(map(lambda x: cv2.resize(x, (IMG_SIZE, IMG_SIZE)), frames)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;31m#             if len(frames) > 5000:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1773252/3759238314.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0;31m#             frames = np.array(list(map(lambda x: cv2.resize(x, (IMG_SIZE, IMG_SIZE)), frames)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;31m#             if len(frames) > 5000:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.5.5) /io/opencv/modules/imgproc/src/resize.cpp:4052: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n"
     ]
    }
   ],
   "source": [
    "location = \"원본/dataset/train/2\"\n",
    "save_path = \"fix_train_data/2/\"\n",
    "file_name = 'features_train_nomal2'\n",
    "preprocess(location, save_path, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-21 01:48:44.502305: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-21 01:48:44.523217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-21 01:48:44.523451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-21 01:48:44.523983: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-21 01:48:44.524456: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-21 01:48:44.524674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-21 01:48:44.524874: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-21 01:48:44.811561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-21 01:48:44.811812: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-21 01:48:44.812021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-21 01:48:44.812219: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10127 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " MobilenetV3small (Functiona  (None, 7, 7, 576)        939120    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 28224)             0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 939,120\n",
      "Trainable params: 927,008\n",
      "Non-trainable params: 12,112\n",
      "_________________________________________________________________\n",
      "frames over :  8873\n"
     ]
    }
   ],
   "source": [
    "location = \"원본/dataset/val/2\"\n",
    "save_path = \"/home/nplab/Jueon/LSTM/datasets/val/2\"\n",
    "file_name = 'features_val_normal2'\n",
    "preprocess(location, save_path, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = \"원본/dataset/val/0\"\n",
    "save_path = \"/home/nplab/Jueon/LSTM/datasets/val/0\"\n",
    "file_name = 'features_train_fight2'\n",
    "preprocess(location, save_path, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "location = \"원본/dataset/train/2\"\n",
    "save_path = \"/home/nplab/Jueon/LSTM/datasets/train/2\"\n",
    "file_name = 'features_val_normal2'\n",
    "preprocess(location, save_path, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = \"원본/dataset/train/0\"\n",
    "save_path = \"/home/nplab/Jueon/LSTM/datasets/train/0\"\n",
    "file_name = 'features_train_fight2'\n",
    "preprocess(location, save_path, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNkJp2QJUm7I2JE2mLAz39U",
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
